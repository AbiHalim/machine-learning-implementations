{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, I am modelling a Perceptron, which forms a simple neural network with one neuron.\n",
    "With this Perceptron, I am trying to model the basic logic gates `AND`, `OR`, and `NOT`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "AND = [(0, 0, 0), (0, 1, 0), (1, 0, 0), (1, 1, 1)]\n",
    "OR = [(0, 0, 0), (0, 1, 1), (1, 0, 1), (1, 1, 1)]\n",
    "NOT = [(0, 1), (1, 0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Some helper functions, including activation functions which can be used,\n",
    "namely the heaviside function and basic sign function.\n",
    "\"\"\"\n",
    "def heaviside(z: int):\n",
    "    return 1 if z >= 0 else 0\n",
    "\n",
    "def sign(z: int):\n",
    "    return 1 if z > 0 else -1\n",
    "\n",
    "def relu(z: int):\n",
    "    return max(0, z)\n",
    "\n",
    "def hw(w: list, point: tuple, activation):\n",
    "    return activation(sum([x*y for x,y in zip(w,point[:len(point)-1])]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The Perceptron Learning Algorithm, default learning rate = 0.1\n",
    "Returns weights\n",
    "\"\"\"\n",
    "def perceptron_learning_algorithmn(data: list[tuple], rate=0.1, activation=heaviside):\n",
    "    n = len(data[0])\n",
    "    data = [(1,) + tup for tup in data] # initialize dummy variable = 1 for each point\n",
    "    w = [0.0 for i in range(n)] # initialize weights = 0\n",
    "    converged = False\n",
    "\n",
    "    for i in range(100):\n",
    "      converged = True\n",
    "      for point in data:\n",
    "          # predict yhat\n",
    "          yhat = hw(w, point, activation)\n",
    "          y = point[-1]\n",
    "\n",
    "          # update weights if misclassified\n",
    "          if yhat != y:\n",
    "              converged = False\n",
    "              step = (y - yhat) * rate\n",
    "              for i in range(n):\n",
    "                  w[i] = w[i] + step * point[i]\n",
    "      if converged:\n",
    "          break\n",
    "    \n",
    "    return w\n",
    "\n",
    "\"\"\"\n",
    "The Perceptron which uses the weights from the Perceptron Learning Algorithm\n",
    "Returns the predicted values of the target variable\n",
    "\"\"\"\n",
    "def perceptron(data: list[tuple], rate=0.1, activation=heaviside):\n",
    "    w = perceptron_learning_algorithmn(data, rate, activation)\n",
    "    data = [(1,) + tup for tup in data] # add dummy variable = 1 to each point\n",
    "    output = []\n",
    "    for point in data:\n",
    "        output.append(hw(w, point, activation))\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Helper function to test the results\n",
    "\"\"\"\n",
    "def test_perceptron(data: list[tuple], predicted: list):\n",
    "    for i in range(len(data)):\n",
    "        yhat = predicted[i]\n",
    "        y = data[i][-1]\n",
    "        correct = y == yhat\n",
    "\n",
    "        print(f\"POINT {data[i]}: PREDICTED = {yhat}, ACTUAL = {y}, RESULT = {y==yhat}\")\n",
    "        if y != yhat:\n",
    "            correct = False\n",
    "    print(\"TRUE\" if correct else \"FALSE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And these are the weights returned by my Perceptron Learning Algorithm:\n",
    "`AND` gate: [-0.2, 0.2, 0.1]\n",
    "`OR` gate: [-0.1, 0.1, 0.1]\n",
    "`NOT` gate: [0.0, -0.1]\n",
    "The first weight in each array is the bias term."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POINT (0, 0, 0): PREDICTED = 0, ACTUAL = 0, RESULT = True\n",
      "POINT (0, 1, 0): PREDICTED = 0, ACTUAL = 0, RESULT = True\n",
      "POINT (1, 0, 0): PREDICTED = 0, ACTUAL = 0, RESULT = True\n",
      "POINT (1, 1, 1): PREDICTED = 1, ACTUAL = 1, RESULT = True\n",
      "TRUE\n",
      "POINT (0, 0, 0): PREDICTED = 0, ACTUAL = 0, RESULT = True\n",
      "POINT (0, 1, 1): PREDICTED = 1, ACTUAL = 1, RESULT = True\n",
      "POINT (1, 0, 1): PREDICTED = 1, ACTUAL = 1, RESULT = True\n",
      "POINT (1, 1, 1): PREDICTED = 1, ACTUAL = 1, RESULT = True\n",
      "TRUE\n",
      "POINT (0, 1): PREDICTED = 1, ACTUAL = 1, RESULT = True\n",
      "POINT (1, 0): PREDICTED = 0, ACTUAL = 0, RESULT = True\n",
      "TRUE\n"
     ]
    }
   ],
   "source": [
    "predicted = perceptron(AND)\n",
    "test_perceptron(AND, predicted)\n",
    "\n",
    "predicted = perceptron(OR)\n",
    "test_perceptron(OR, predicted)\n",
    "\n",
    "predicted = perceptron(NOT)\n",
    "test_perceptron(NOT, predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, I will try creating a Multi-Layered Neural Network by combining multiple Perceptrons. Then, I will implement the `XOR` gate, which can't be implemented with only 1 Perceptron as the data is not linearly separable.\n",
    "\n",
    "![gatexor](gatexor.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "XOR = [(0, 0, 0), (0, 1, 1), (1, 0, 1), (1, 1, 0)]\n",
    "\n",
    "weights_and = perceptron_learning_algorithmn(AND)\n",
    "weights_or = perceptron_learning_algorithmn(OR)\n",
    "weights_not = perceptron_learning_algorithmn(NOT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing XOR Gate:\n",
      "(0, 0) → 0\n",
      "(0, 1) → 0\n",
      "(1, 0) → 0\n",
      "(1, 1) → 0\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Generate Intermediate Data\n",
    "xor_training_data = []\n",
    "for a, b, target in XOR:\n",
    "    or_out = hw(weights_or, (1, a, b), relu)\n",
    "    and_out = hw(weights_and, (1, a, b), relu)\n",
    "    not_out = hw(weights_not, (1, and_out), relu)\n",
    "    xor_training_data.append((or_out, not_out, target))\n",
    "\n",
    "# Step 2: Train Final Layer\n",
    "weights_xor = perceptron_learning_algorithmn(xor_training_data)\n",
    "\n",
    "# Step 3: Prediction Function\n",
    "def predict_xor(a: int, b: int, activation=heaviside) -> int:\n",
    "    or_out = hw(weights_or, (1, a, b), activation)\n",
    "    and_out = hw(weights_and, (1, a, b), activation)\n",
    "    not_out = hw(weights_not, (1, and_out), activation)\n",
    "    return hw(weights_xor, (1, or_out, not_out), activation)\n",
    "\n",
    "# Step 4: Test\n",
    "print(\"Testing XOR Gate:\")\n",
    "for a, b, _ in XOR:\n",
    "    print(f\"({a}, {b}) → {predict_xor(a, b)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs2109s-ay2425s2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
